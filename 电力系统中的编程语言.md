经济学领域的“不可能三角”挺火，类似这种提法，编程语言的三个需求不可能同时满足：

1）很高的性能要求；

2）较低的学习和开发成本（包括无代码/低代码的快速搭建方式）；

3）自主可控，不依赖国外的商业平台。

其实我也幻想过，有没有可能存在某种技术方案，可以把这三条都满足了。但是，自从学习了计算机系统体系结构有关的知识后，发现确实是不可能。当然，如果基于某个强有力的商业平台，例如MATLAB，确实可以实现较高的性能和较低的开发成本，可以同时在工业界和学校使用，但是短期内，国内不可能有替代的方案。

下面依次介绍电力系统中的常用编程语言。

1  C/C++

C和C++确实是两种完全不同的语言，不能认为C++是C的超集。但是这两者的联系很紧密，我就放到一起了。电力系统首先主要是工业，既然是工业，那自然是嵌入式的装置研发比较多，嵌入式装置以C语言为主。就算是现在工业物联网、主站信息系统的发展，云计算、大数据、人工智能在电力系统应用也越来越多了，但这些云上的东西，C++的成分依然比较多。特别是电网调度控制的一区，不允许使用国外开源软件，大家基本上用C++开发控制类型的应用（如SCADA、AVC、AGC）。另外，电力系统的界面，目前大多数是基于Qt开发的(Qt是基于C++写的库)，虽然有往 Html5 的方向发展。

2  JAVA

JAVA是国内应用最广泛的语言，从业者最多。大数据平台、分布式计算机系统的开发以JAVA为主。网上有种说法：“你如果不懂JAVA，那就与平台架构师无缘了”。当然，很牛的公司，例如谷歌、腾讯，还是以C++为基础构建大数据平台的，但这样的公司不多。

3 Python

人工智能的发展很快，大多初学者都用Python。Python是对初学者最友好的语言，现在中学生也逐渐开始学习编程和人工智能技术了，我翻看了他们的教材，都是使用Python的。Python的问题是性能不好，很难适应工业级程序的开发，但是它作为胶水语言，写一写调试的小工具、小脚本是最好不过的。我调试部分协议的socket接口，或者研究过程中绘制一些波形曲线图，用的是Python语言。

4  其它

Go语言是替代JAVA的候选，其上手容易，并发性能也很好。

C#是微软旗下的语言，上手容易，特别是桌面的开发非常方便。

Fortran是最古老的语言，适合科学计算。我刚工作那会，还调试了不少Fortran语言呢，当时的电力系统潮流、状态估计程序都是用Fortran写的，但最近几年Fortran好像逐渐淘汰了。

Rust是唯一与C++对标的语言，其内存安全性得到了很好保证，但目前用户不多。

Juila是科学计算的语言，性能比Python高，是Python的强劲竞争对手。

总结：从我的经验来看，C/C++仍然是电力系统最主要的编程语言，学习难度也最大。对于研发人员，除了C/C++，建议再学至少一种语言作为互补，这对于思维方式的开拓很有好处，具体哪个就不强求了。

谈论信息技术与编程语言的发展趋势时，程序员习惯从技术本身出发，较少考虑外部环境和社会的发展变迁。比如说，经常看到Java程序员说：”程序员比机器更贵，因此节省程序员开发时间的Java语言必将长盛不衰“。如果以发展的眼光会发现这种观点很有问题，程序员长期比其他脑力工作者收入高的现状很难维持，未来的极大可能趋势是程序员收入的相对下降，这在经济学中很容易解释的。一方面，AI大模型的发展会使得程序员的马太效应更加显著，随着AI的发展和自动化水平的提升，某些编程工作可能会减少，从而影响程序员的需求；另一方面，信息技术与其它传统领域的结合（数字化转型），会使得程序员供给增加，各个行业都会着力培养自己行业的程序员。考虑这种趋势，会发现程序员比机器更贵的趋势将来可能会扭转。

与此同时，自然环境的约束，会使得人类社会更加重视节能减排，以满足碳排放的控制。这种趋势是强有力的，在电力行业已经产生了几乎颠覆性的影响，很快也要在计算机行业产生更大的影响。在传统上，信息技术工作者较少考虑供电的问题。比如说，在5G研发之前，通信行业的工程师几乎没有人预测出5G基站的耗电竟然如此之大（虽然从物理学上分析出5G更耗电是不困难的）。根据网上的一些报道，全国5G基站电费增加数百亿元人民币，几乎占据利润的大部分。

根据国外的统计数字，数据中心和数据传输网络分别占全球用电量的 1-1.5%。对于个人和工商业办公用到的个人电脑、手机的耗电量一直缺乏细分的数据，这部分都算在工商业用电和居民用电内部了，包括暖气、冷却、照明、烹饪、家电，等等。可以认为，分散的个人信息设备、嵌入式设备，总用电量应该超过数据中心，因为数据中心的集中管理会使得能效更高。因此，全球的总信息设备用电量，可能会接近5%，这已经是非常巨大的电力消耗了。在未来，人工智能应用如果进一步发展，可能会有更大的比例。

事实上，在信息技术方案设计方面，能效已经是影响很大的因素。比如在手机领域ARM芯片的快速推广；在数值计算领域，GPU不仅更利于浮点计算，而且能效超过CPU，这些都是显著的技术趋势。

下面从软件开发的角度来分析能效因素。关于不同编程语言的能源消耗，2017年有篇论文影响很大，题目是《Energy Efficiency Across Programming Languages》，作者通过标准化算例分析了不同编程语言的性能、能耗、内存占用。最重要的指标当然是能耗。

根据论文的数据，总体上，编程语言可以分为三类，分别为编译型语言（代表是C/C++/Rust）；基于虚拟机的编译型语言（代表是Java/C#）；基于虚拟机的解释型语言（代表是Python/PHP）。假定C语言的相对能耗是1.0，则编译型语言的能耗接近1；基于虚拟机的编译型语言能耗为2~3；基于虚拟机的解释型语言能耗大约为30~80。

由于是标准化测试，各种指标是接近的。对于真实的大型工程项目，不同类型的编程语言可能差距会更大。这项研究表明，编程语言的选择可以显着影响能源消耗，C、C++ 和 Rust 等编译语言通常比在虚拟机上运行的 Java 等语言或 Python 和 Perl 等解释语言更节能 。

上述结果是符合计算机体系结构原理的，计算机的耗能80%由CPU消耗。编译型语言便于控制内存布局、进行性能方面的优化，提高CPU的运行效率。C语言已经成为事实上的低层语言标准，如果我们进行系统级编程，那么零开销抽象就非常重要，这方面C++ 和 Rust是候选。

除了编程语言，信息技术在以下方面也要注意能效：

1）硬件选择：选择低功耗的处理器和传感器，如ARM架构的处理器，这些通常在物联网设备中使用以降低能耗。在科学计算方面，GPU大量使用；在数据中心，FPGA也用的越来越广泛了。2）算法优化：开发高效算法，减少计算复杂度和内存使用，从而降低能耗。3）系统级优化：利用操作系统级别的能效管理特性，比如动态电压和频率调整，休眠模式等。4）能源采集：对于某些设备，如移动基站或边缘设备，可能考虑使用太阳能等可再生能源。

在数据中心方面，需要优化数据中心的冷却系统和能源使用效率，使用更多的可再生能源，包括使用更高效的通信协议和网络设备。在寒冷地区，数据中心可以利用外界低温来提高冷却系统的效率，因为外界的低温空气可以直接用于降低内部设备的温度，从而减少机械制冷所需的能量。这是为什么很多大型数据中心会选择在气候较冷的地区建设，以此来降低冷却成本和提高能效。当然，数据中心靠近水电站也是一个可行的想法，可以减少长距离电能传输过程中的损耗，同时有充足的水资源进行直接或间接冷却。

最后谈一谈网络通信的能耗影响，由于现代交换设备和网卡工作的原理，CPU需要付出大量资源进行数据包的快速接收处理，这方面的能耗是不可忽略的。同时，带宽也是资源，也有很多建设费用。因此，在数据产生的地方（如边缘计算）进行更多的数据预处理和分析，减少数据传输需求，实现信息处理的本地化，也是一个趋势。

本文主要讨论高性能编程，而且是那种“极致性能需求”。按照本人的粗浅认识，应该已经覆盖了绝大多数技术要点，但缺点是不够详细（毕竟是公众号文章，篇幅有限）。本文共分为4个部分：总体论述、高性能网络编程、高性能数值计算、常规高性能需求。

1）总体论述

C/C++的重要性，在于所有操作系统内核都是C语言编写，嵌入式设备编程也基本都是C语言。虽然最新版本的Linux内核在讨论是否引入Rust。但未来10~20年，直接操作硬件的编程语言仍然是以C语言为主的。C++是对C兼容性最好的高级语言，它可以保留对硬件底层操作能力的同时，提供“零开销”的抽象能力，从而可以使用更抽象的软件工程成果。在所有高性能需求场合，C/C++是最重要，某种程度上也是唯一的方案。

下面简略谈一下别的编程语言。

Fortran 在数值计算领域的历史要早于 C，它一直是科学和工程应用的主要语言。但是Fortran 脱离计算机工业主流发展，一个大趋势是Fortran 的科学计算库逐渐被改为C++语言。比如说我刚工作的时候还是写过Fortran 代码的。但是工作5~6年以后，所有的Fortran 代码基本都改成C++了。

Julia 是专为高性能数值计算而设计的编程语言，比较适合科研。Julia 比较适合对性能有较高要求，同时不熟悉计算机体系结构的科学家或工程师使用（比如物理学、电气工程、气象预报、经济学等非计算机专业）。本文讨论的是极致性能需求，和较高性能要求还是有本质区别的。

Rust是除了C++，唯一既具有底层操作能力，又具有抽象能力的编程语言。Rust放弃了对C的兼容，在内存安全性、编译器等方面做了巨大改进。C++与Rust的对比经常是技术论坛争吵的话题。我个人认为相当长时间内，Rust仍然不能撼动C++的地位。

Java是主流编程语言。一般情况下，Java的性能比C++慢10倍，内存占用可能超过10倍。所以，Java是不能满足高性能需求的。比Java更慢的语言，例如Python，就更不适合高性能需求了。有人可能觉得用Python做AI应用，性能也很不错。那是因为你调用的别人写好的库，库的内部多半是用C++实现的，相当于在别人的地基上搭建自己的房子。

高性能编程需要有一个总体、历史的视角。大约2006年起，CPU的摩尔定律差不多“失效”了。在那之前，主频常常从一个版本跃升到下一个版本，从几百兆赫兹飙升到几G赫兹。但到了2000年代中期，这种飞速的增长放缓了，主要是由于功耗和热量问题。从那时起，CPU的升级主要是通过增加核心数、提高每个核心的效率、增加缓存大小等方法来提高总体芯片性能。但是不经过任何优化的程序，是在单个CPU核心上运行的，这是最吃亏的。这意味着常规程序不能享受信息工业的进步。

相对于CPU，其它硬件设备进步反而更大。比如说网卡，从 1Gbps 的速度（2000年左右）到 10Gbps、25Gbps 甚至 100Gbps(2014年左右)，网卡的传输速度得到了显著的提升。甚至网络协议中的部分校验功能，也可由网络硬件来实现了。比如说GPU的最新发展，其浮点计算能力几乎超过CPU的10倍以上；还有FPGA、ASIC芯片的发展，很多特定的功能可以交给硬件实现。当然，非CPU设备专门用于特定任务，通用编程仍然是CPU完成的。因此，C/C++的编程，越来越像是“指挥官”的角色，把各种各样的硬件协调好、发挥最大功效。因此，真正高性能的系统一定是软硬件结合的。

对于传统的纯CPU程序，高性能优化的主要方向是多核并行（或者是用户空间上下文的快速切换例如协程技术），这同样需要深入的计算机组成原理的知识，比如存储的层次结构、多核下缓存的一致性问题、函数的汇编实现等。

传统上来说，“指挥官”的角色是由操作系统内核来承担的。即使是计算机专业的毕业生，也少有人对操作系统内核十分熟悉。我听说很多高校的操作系统课程是以讲授理论为主，很少有学生能深入真实的操作系统内核进行编程和调试的。这个现状已经越来越难以适应高性能技术的发展了。

下面再谈一下若干纯技术问题。

时钟测试：这一点非常重要，很多人优化程序凭借自己的“感受”，其实主观感受经常非常不准。根据Amdahl定律，只有测试出最占时间的串行代码，并行优化才能取得最大功效。Linux经常采用gettimeofday函数，被认为是比较精确的时间测量函数。这个函数早期是系统调用，会产生较大的代价。但现在新的内核版本，gettimeofday函数已经是用户空间函数了，或者至少避免了上下文切换。除了gettimeofday函数，更精确的函数是rdtsc，这本质是汇编语句封装，直接读寄存器，获取CPU时间戳，每一个处理器时钟周期，它就增加 1。然后这个差值除以CPU频率就得到时间。使用 rdtsc 也需要注意一些技术要点：固定CPU频率，在高性能应用程序中，通常建议禁用CPU的动态调整功能，特别是在BIOS设置和内核裁剪编译两个环节都要禁止电源管理（防止降频）；绑定线程到特定的CPU核，同时进行CPU亲和性设置，防止高性能线程的中断；测量时间过程中尽量不要进行系统调用，包括不限于：延迟和定时；文件和阻塞式网络I/O；同步和锁定；进程和线程管理；内存申请释放，等等。

高性能指标的矛盾：实时性是为了在给定的时间约束内完成特定的工作。例如，硬实时系统必须在固定的时间内响应，否则可能会导致系统崩溃或其他不良后果。吞吐量是系统在单位时间内可以处理的工作量。为了确保任务在固定的时间内完成，可能需要预留更多的计算资源（例如把特定的CPU核独占），这可能会降低系统的总体吞吐量。反之，为了最大化吞吐量，可能需要允许某些任务的完成时间超出其理想的时间范围。具体优先优化何种指标，需要根据实际的需求来定。还有一些其它的矛盾指标，例如功耗与性能、价格与性能等等。

2）高性能网络编程

大多数实时的需求都建立在与外界通信基础上，比如说工业控制、量化交易、硬件在环仿真等。毕竟数字世界与真实世界交互，网络是最常用的交互方式。其它的接口，例如RS-485工业总线，在通信速度方面可能弱于以太网，但抗干扰能力远强于以太网。在电力系统我接触的范围，硬实时都是通过以太网或者无源光纤接口实现的。因此，本文的高性能网络编程默认为以太网网卡（或者光口）。

前面说过，传统网络编程任务全部交给内核。但内核是通用的，对于高性能网络通信任务是不擅长的。内核socket的各种系统调用、上下文切换、内存拷贝等，没有专门的高性能优化，这些都会增加延迟并减少吞吐量。假设我们把内核比作大山，山上确实有非常多的风景，但我们的目的并不是欣赏风景，而是快速通过。那么无非是两种方法：一是打隧道，也就是进行内核编程；二是修路从山旁边绕过去，也就是By-pass 技术，绕过操作系统的传统网络堆栈，直接在用户空间中处理网络数据包。

内核编程方面，Linux内核的新版本已经有很多改进，例如零拷贝(Zero-Copy)，传统的数据传输需要多次的内存拷贝，这会消耗大量的CPU资源。零拷贝技术通过减少或消除这些拷贝操作来提高性能。还有非阻塞I/O、I/O多路复用，例如select、poll和epoll，它们可以让单一的线程监视多个文件描述符，有效地管理大量的并发连接。对于上层的应用程序，我们一般不直接调用socket，而是使用封装的网络库，例如boost::asio库，是非常好的工具。但充分发挥库能力的前提是对内核处理网络通信的过程，不能一无所知。

在嵌入式开发需求方面，即使是升级后的Linux内核都不能胜任了，我们可以进行内核裁剪、编程，自己增加特定的系统调用（或者驱动）。我所知的，有人把IEC 61850的实现放在内核中完成。其实IEC 61850实现本身并没有复杂到哪里去，主要是内核的开发调试需要掌握的知识点非常多。

另外一个技术路径是By-pass 技术，在用户空间中处理报文，减少上下文切换，避免不必要的内存拷贝。许多 by-pass 解决方案使用轮询模式来检测新的数据包，而不是依赖中断。这可以减少中断的开销，尤其是在高流量环境中。还有一点是很关键的（但是从未看到别人提到）：用户态程序开发调试的麻烦程度是比内核小太多了，可能小10倍都不止。

DPDK是By-pass 技术的典型代表，最近一段时间我一直在调试DPDK，充分感受到DPDK的博大精深，它的性能强到让人吃惊的地步。DPDK使用大页内存和高效的缓冲区管理，以及零拷贝、轮询等技术，能够快速地处理和转发数据包。DPDK完全接管网卡，所以网络协议栈必须自己写，会遇到很多socket编程根本设想不了的问题。

3）高性能数值计算

前面提到，数值计算方面在GPU的推动下，“异构”已经是主流编程方式了。首先，高度并行的、计算密集型的部分适合 GPU，而 I/O 密集型、分支密集型或需要复杂数据结构的部分适合 CPU。很显然，不是所有的任务都适合GPU来做。除此以外，CPU 和 GPU 之间的数据传输可能会成为瓶颈。尽量减少数据传输的次数，尤其是在频繁的计算迭代中。使用异步数据传输，允许 CPU 和 GPU 同时工作，而不是等待数据传输完成。大多数情况而言，到达微秒的实时性要求级别，GPU不太适合了，这种情况下更多的是使用FPGA。另外，多利用成熟的库，如 CUDA、cuBLAS、cuDNN 等，它们经过优化，可以提供很好的性能。

对于纯粹基于CPU的高性能计算任务，优先采用高性能的库。例如，MATLAB的底层矩阵库就是MKL（稠密矩阵）/SuiteSparse(稀疏矩阵)。这些高性能库千锤百炼，有大量的优化甚至汇编优化、大概率比你自己写的计算程序要好。只有在极其狭窄的功能或者场合，自己写的库要更好些。比如我自己写的稀疏矩阵加法的性能比Eigen略好一些。

我自己用过这些数值计算库：MKL（但是其稀疏矩阵的计算性能比较让人失望）；SuiteSparse(Tim Davis教授的经典库，稀疏矩阵，最近增加了图计算库）；Eigen（比较好的C++矩阵库）；OpenBLAS(张先轶的作品，朋友圈有他)；GLPK(优化算法库)；fftw（快速傅里叶分析）。

如果是非常大型的高性能计算，需要动用数据中心的力量，甚至是不同城市的数据中心。这就是分布式系统架构的概念了，例如OpenAI公司已经使用Kubernetes训练自然语言大模型。

4）常规高性能需求

对于常规高性能需求，主要是日常编程的时候有些技术准则要遵循。《深入理解计算机系统》（第三版）这本书已经总结得很不错了，主要包括：循环的时候要注意流水线问题（第4章），减少分支预测失败，排好序的循环性能会提高很多；充分利用编译器（第5章），包括让编译器实现SIMD并行与循环展开、减少不必要的内存操作、减少过程调用；理解存储的层次结构（第6章），充分利用局部性原理和提高缓存命中率，这点可能是对常规软件影响最大的方面，JAVA无法像C/C++那样控制内存分布，这就导致进行不了这么深入的优化。这本书后面还讨论了系统调用、IO、网络编程等内容，都对高性能编程有很好的参考价值。《深入理解计算机系统》写得非常好，也很基础。这本书完全掌握后，才谈得上对高性能编程有些概念（可能还算不上入门）。

对于C++编程，还有一些准则需要遵循。这方面可以寻找专门C++的专著来看。C++的特殊之处是编译器可能在程序员背后做很多事情，有时候会让程序员大吃一惊。特别是C++中的字符串、vector等标准库设施，在内存申请之前一定要reserve；还有避免不必要的复制，这在初始化、函数传参等方面都要注意，可以适当使用移动语义。C++的并发编程也一直是很难的问题，稍微不仔细就会遇到性能损失或者漏洞，可以谨慎的使用原子操作和无锁数据结构。

当然，数据结构和算法还是很重要的，但这已经不属于“极致优化”，而属于程序员的常识。我经常看到同事写多重循环，每个循环都要从头到尾遍历一遍，每当看到这样的代码都感觉有些心痛，大量的CPU周期和数据中心电力都这样被浪费了，而且确实遇到过算法复杂性没做好导致的现场问题。C++的标准库在数据结构和算法是非常经典的，这点无可置疑。另外，还有一些库是对C++标准库的进一步优化，例如folly库。

1）在计算机系统中，硬件和软件是相互依存的两个方面。无论是最简单的个人计算机还是最复杂的超级计算机，它们都需要这两个组件紧密协作，才能完成各种任务。例如，操作系统软件协调硬件资源的使用，应用软件则提供特定的功能，如文字处理、图像处理或数据库管理。即使是FPGA，似乎是纯粹的硬件，它的设计是离不开EDA软件工具的。

这一点与电力系统是一致的，硬件是电力设备和二次设备中的装置平台，同样需要软件的组织，与管理体系和经济社会连接，才可以作为整体运行。

2）曾经由软件执行的大量计算任务，渐渐转移到专为这些任务设计的硬件上。显著的例子是图形处理单元（GPU）。最初，计算机图形主要由CPU处理，但随着图形处理要求的提高，GPU应运而生，专门用于处理复杂的图形和图像计算。这不仅极大地提高了处理效率和性能，还减轻了CPU的负担，使其可以处理其他任务。这种“硬件化”的趋势也出现在了网络处理、数据加密、音视频处理等领域。但是新的需求往往是先在软件层次实现的，直到成规模后才考虑硬件化。硬件和软件是此消彼长的过程。在过去的10多年时间，软件技术更加活跃。但是近期，乃至未来的若干时间，反而是硬件发展更快，需要软件体系迅速改变（尤其是底层基础设施），跟上硬件的发展。

 在电力系统方面，同样存在一次系统投资建设与二次设备（包括自动化和信息化系统）之间此消彼长的过程，不同的历史时期重点是不同的。

3）在计算机系统的发展中，协议扮演着至关重要的角色。协议定义了不同计算机组件、系统、网络及其它设备之间交流的规则和标准。这一点对于硬件和软件的协同工作尤为重要。协议确保了来自不同制造商的组件和系统能够无缝对接和交互，从而支持了一个多样化且互操作的计算生态系统。例如，网络协议如TCP/IP为不同类型和品牌的设备之间的通信提供了标准化的方法，而数据传输协议如USB和PCI-E则确保了各种类型的硬件设备能够有效地相互连接和通信。协议制定好后，不同的厂家都可以各自精益求精的专心研发，然后组合成完整的系统。

在传统上，电力系统的技术承包商竞争不激烈，因此协议的重要性显著不如计算机系统。但是未来，随着电改的深入推进，电力系统的开放性必将大幅度提升，未来的电力系统协议，以及不同技术厂商和甲方的交互，将更加显著。


1   现代C++的定位

当前C++的应用场合已经比较少，至少不是热门的技术。即使是重点大学的计算机专业学生，很多人也不会把精力用于学习C++语言。但是在目前的阶段，我们依然不能说C++已经属于被淘汰的技术，因为C++语言更多成为信息世界的基础设施。

计算机和信息技术为什么可以成为新一次的科技革命？首要原因是计算机技术大大推进了生产力的发展。回看历史，计算机和网络技术是先用于军事，然后用于工业生产，最后再应用于娱乐和消费端。你不能说消费比生产更重要，毕竟只有消费没有生产，人类立刻就要灭亡。但是中国的计算机行业由于发展阶段，以及国际分工等原因，主要只发展消费端和娱乐。对于计算机底层和应用于工业生产，是严重缺失的。按照我粗略的估计，工业界的程序员数量恐怕只有消费端的10%吧，收入也低。

如果软件工程只局限于消费、娱乐，那C++确实不是好的选择。但是如果是做工业软件，C++作为系统性编程语言，是非常合适的。拿我现在的工作来说（国产的实时仿真装置），C++几乎是唯一的选择。

1）首先，有硬实时的要求，这就直接排除python和JAVA了，Julia语言也达不到硬实时的要求。

2）其次，有大量的科学计算，哪怕是计算效率比竞争对手高10%,那都是了不起的优势，因为确实可以降低用户的成本。在高性能科学计算方面，似乎C语言、C++、Fortran都可以胜任。但是从软件工程方面，C++多范式的特点，用的好的话可以提高研发效率。比如说仿真某个装置，几十个型号都需要开发相应的模型，用C或者Fortran怎么行？很快代码就要变成一团乱麻了。Rust也是系统编程语言，但是Rust的优势是减少内存bug，在科学计算方面还未听说Rust也很强的。

3）有半实物仿真的需求，这就要求仿真软件能够无缝对接各种驱动，甚至深入到操作系统内核，而不是仅仅调用操作系统提供的接口。例如，连接FPGA装置需要走PCI总线，还有链路层的网络报文（无法使用TCP/IP协议，因为性能达不到要求）。由于操作系统和驱动都是C语言写的，因此在这方面C和C++都符合要求，其它语言都增加了间接的一层封装。

综上，对于工业软件来说，C++是非常合适的，很多时候也是唯一的选择。

2  C++的设计原则

这部分很多是摘抄网上的说法，加上一些个人的经验，只可以参考，不一定适用所有的场合。

2.1 系统设计的产出物，应该是一个原型程序

  我不止一次看到这样的案例：某个复杂业务系统，几十个人进行设计，设计的似乎完美无缺，无论是上千页的设计文档，还是精美的PPT，都说明这个系统会是尽善尽美的。但是经过开发、工程投运之后，用户却发现这个系统完全不好用，就算用的起来，那也是磕磕绊绊、投入大量的人力去维护。

这说明设计和工程实施出现了脱节。问题之一是设计人员中或许有业务专家、或许有计算机高手，但是没有兼具一线业务与计算机专业知识的人，造成设计想法的难以落地。其次是，设计是容易滥竽充数的，不像软件开发有那么多的测试工具、静态检查工具、代码审查等。有的人，连Python都不会写，就可以为复杂C++程序进行设计了，这肯定会出大问题啊。网上有很多讽刺“PPT架构师”的段子，建议大家去看看。

为了避免架构的脱离实际，我建议设计的产出物，至少是一个可运行的原型程序。这个原型程序可以没有界面和实质性功能，但是至少可以显示业务流程。这也是为后续实质性的研发，打下一个很好的基础。当然设计文档也是需要的，但是没必要对设计文档提出过多的形式化要求。把设计写在黑板上，用手机拍下来，如果研发人员都觉得看得懂、思路清晰，那就可以作为设计文档。关键是解决实际问题，解决问题是第一位的。

2.2 具体编程的注意点

  Don’t Repeat Yourself：当在两个或多个地方发现一些相似代码的时候，我们需要把它们的共性抽象出来形成一个唯一的新方法，并且改变现有地方的代码让它们以一些合适的参数调用这个新的方法。

  最少知识原则：一个类对于其他类知道的越少越好，就是说一个对象应当对其他对象有尽可能少的了解,只和朋友通信，不和陌生人说话。反面教材：有人喜欢用全局变量，写的时候确实很爽，随心所欲。

单一职责原则：一个类，只做一件事，并把这件事做好，其只有一个引起它变化的原因。职责过多，可能引起它变化的原因就越多，这将导致职责依赖，相互之间就产生影响，从而极大地损伤其内聚性和耦合度。把模块间的耦合降到最低，而努力让一个模块做到精益求精。使用多个专门的接口比使用单一的总接口要好。反面教材：有的大型系统经常出现几万行代码甚至更多行的超级大类，这种类很快陷入不可维护的境地。如果作者转岗、辞职，其它同事对于这些代码只能干瞪眼，很多时候只能推倒重来。

开闭原则：模块是可扩展的，而不可修改的。也就是说，对扩展是开放的，而对修改是封闭的。对扩展开放，意味着有新的需求或变化时，可以对现有代码进行扩展，以适应新的情况。对修改封闭，意味着类一旦设计完成，就可以独立完成其工作，而不要对类进行任何修改。反面教材：很多程序加一个简单功能都需要修改几十个地方，用户或者领导非常困惑：加的功能太简单了，看样子几分钟就能开发出来，为什么他们说至少需要1个月呢？

关注点分离：问题太过于复杂，要解决问题需要关注的点太多，而程序员的能力是有限的，不能同时关注于问题的各个方面。实现关注点分离的方法主要有两种，一种是标准化（大家都按照标准行事，最后拼装成大系统），另一种是抽象与封装。

依赖倒置原则：高层模块不应该依赖于低层模块的实现，而是依赖于高层抽象。在依赖结构中不允许出现环（循环依赖）。这就需要对软件的层级结构很好的设计，绝不允许出现混乱。有时候临时一个需求，随手就开发出来，看样子工作能力强、业绩优秀。但是却破坏了整体架构，久而久之就不可收拾了。

除了上述要点，写大型C++程序，测试也是非常重要的。有的人甚至推荐测试代码要先于业务代码去编写。有的业务，测试代码的行数比业务代码还多。测试驱动有专门的方法论、工具与管理措施，我这里就不展开说了。

3  C++的编程范式

3.1 过程式(C语言范式)

使用C++编程，很难回避与C语言的混合编程，这是因为这两种语言天然接近底层，而操作系统和驱动，以及许多高性能的库，是C语言编写的。C语言本身的范式属于直接实现设计思路，语言与汇编严格对应。编译器很少会做出让你大吃一惊的“优化”。C++编程在很多场合，例如与C语言的混合编程，以及某些特殊需求，是有必要采用C语言范式的。

我有个观点，不知道对不对，就是你用C++如果完全用不到指针，那真的不如转到Java等开发效率高的语言，指针是C语言最显著的标志之一。一旦与驱动、内核打交道，指针就是无论如何也绕不过去的关键概念了。你用python等语言，觉得没指针也好好的，那是因为某种中间层（JVM、CPython等）屏蔽了底层，这样也好，提高了很多开发效率。我有时候写高性能的仿真程序，要求时钟控制在微秒级别，很多时候不得不用裸指针，有时候也不得不改写Linux内核程序，虽然看起来这种做法严重违反了现代软件工程的推荐做法。

当然在用户态编程，如果不涉及操作系统内核，那还是有必要用智能指针包装一下的。有人提出操作系统内核其实也可以用智能指针。

3.2 基于对象范式

    基于对象更多的是用类进行封装，但类与类之间没有继承、多态等复杂关系。关于类，大约可以分为两种：1）一种类存在对资源的抽象封装和管理，例如动态内存、文件句柄、socket等，这种类需要仔细安排资源管理，定义或者禁用拷贝构造函数；编写析构函数，这就是著名的RAII（资源获取就是初始化）原则；2）另一种类并没有对资源进行管理的职责，可以直接复制，例如矩阵、字符串等，这种类就没有必要实现“深拷贝”，没必要写析构函数。

    把类的作用分别搞清楚，就可以解决大多数关于类的业务问题。

3.3 面向对象

   面向对象是很多教科书，特别是设计模式方面的书详细介绍的。优点：大量的经验（设计模式），符合人的直觉，与真实世界对应。缺点：太多的封装，代码一层层套；重构困难；虚函数的运行效率会略有下降；使用虚函数会有二进制兼容性问题(与C语言不兼容，C++编译器会自动安排虚函数表及虚函数指针，这是用户无法看见的)。

   有的人对面向对象严厉批评，比如陈硕、王垠等人，觉得是“花拳绣腿”；当然也有人，例如罗剑锋、吴咏炜等人持中立的态度。

我觉得，用不用面向对象，需要具体问题具体分析。例如设计模式中的组合模式，用基类加继承是合适的。但一定不能滥用，毕竟使用C++的首要保证高性能，至少不能像JAVA那样滥用面向对象。

顺便提一句，用C语言（包括嵌入式）也可以实现面向对象的特性，只需要把函数指针表处理好就可以。

3.4 泛型/模板

如果说面向对象是运行时候的多态，模板就是编译时候的多态。其中STL对于数据结构和算法的解决，是泛型极其精彩的应用案例。STL的设计与面向对象是毫无关系的，有些初学者可能把C++当成面向对象的语言，其实不是的，C++是多范式语言。

泛型/模板、编译期编程，还可以用“模板元”的方式，这部分抽象程度太高，至今我也不会，就略过不谈。

3.5 函数式

  C++11引入的lambda 表达式，使C++逐渐具备函数式风格。函数式可以理解为更高抽象层次的泛型，它可以把函数打包成变量，然后就可以实现“函数的函数”。在许多场合，函数式可以极其简化程序，例如设计模式中的行为模式，可以用函数式的风格，避免使用继承。对于STL的有些应用，使用函数风格，可以降低循环的出现次数。

  函数式编程期望函数的行为像数学上的函数，要点在于：

1.会影响函数结果的只是函数的参数，没有对环境的依赖；

2.返回的结果就是函数执行的唯一后果，不产生其他影响；

3.函数就像普通的对象一样被传递、使用和返回；

4.代码为说明式而非命令式，可读性高。

但是，走到极端也是不行的，不能为函数式而函数式、滥用函数式。我觉得，函数式可以是其它范式的一种补充。

4  C++推荐书目

   关于读技术书籍，尤其是C++方面的极端强调实践的技术书籍，有以下原则：

一是明确读书的目的是为了用于实践。好的技术书籍来源于工程实践，那么读者读完之后用于实践，才是一个最合理的闭环。如果读书的目的是为了考试得个好成绩，又或者想把论文包装的高大上，又或者想增加些炫耀的资本，那就有失偏颇，而且未必能有预想中的效果。即使是从掌握知识的角度来说，学以致用也是最高效的，死记硬背是低效的。有时候，因为条件所限，暂时找不到合适的应用实践环境，怎么办呢？其实把所学的知识整理之后传授给他人也是一种实践活动。如果没有人愿意来学，自己整理并系统化，也比被动的学习效果好的多。

    二是分清主次。把知识点详略程度和重要性划分一个坐标系，那么会存在四种区域。区域一：重要且简略的知识。例如面向对象设计的几个设计原则（如里氏替换、多用组合少用继承等）。这部分知识当然是重要的，但是过于强调则容易陷入教条主义，脱离实际。区域二：重要且详细的知识。主要是涉及到重要的实操、案例、说明、推导等。这部分知识当然要详细掌握，但不能只见树木不见森林。区域三：不重要但详细的知识，包括许多细节，随着技术进步可能会被淘汰的部分。这部分最好是随着实践自动掌握的，不值得花脱产的时间去学习。有的人特别喜欢深究这部分知识，美其名曰抓细节、性格仔细认真，其实陷入牛角尖而不自知。区域四：不重要也不详细的知识，例如花絮、人物生平、补充说明之类的，但如果想深入进阶，有的地方还是不能略过不看。读书时要会分辨知识是属于哪个区域的。

三是及时复盘。我读《深入理解计算机系统》，其第一章讲的是C语言hello world程序执行所发生的背后的事情。第一次看的时候觉得好神奇（特别是对于我这样的非计算机专业的人来说）。但是这本书仔细读过一遍重新看，只觉得第一章写的简直酣畅淋漓，真的是高屋建瓴，非常自然（看山还是山，看水还是水，hello world还是hello world），对作者的佩服更加深了一层。

  下面是具体推荐的C++书目。首先是基础类：

1）BjarneStroustrup 著.A Tour of C++, 2nd ed 中文版：王刚译，《C++ 语言导学》(第二版）。机械工业出版社，2019。这是一本小册子，简明扼要的介绍了现代C++（C++17标准，后面应该会再更新）。篇幅不长，但是极其权威。

2）Bjarne Stroustrup 著.《C++程序设计原理与实践》。这是C++之父专门为大一新生写的教科书。当然难度比真正的“教科书”难得多。但不管怎样，C++之父百忙之中为初学者写书，是非常难能可贵的。这本书明显看出来大师跟普通的C++教师的差距，即使是面向初学者，书里面也涵盖了矩阵计算、嵌入式编程、GUI编程、测试等实用性内容。这本书对于C++教师也是很有参考意义的。

  3）《C++ Primer》（第五版）。这可能是销量最大的C++教科书。问题是只支持到C++11标准，据说很快有新的版本。读这本书，不用担心作者讲错了，可以说毫无瑕疵。里面的案例很值得揣摩，比如文本查询程序。几乎每个章节的示例程序，都值得敲到电脑里面执行、揣摩。我在工作中看到，很多程序员被一些bug卡死，有的bug几天都解决不了，仔细读《C++ Primer》往往直接就可以避免。

  4）《C++ 标准程序库》。C++标准库是一个宝库，虽然很多人诟病C++标准库的内容还是不够多（例如没有网络库）。但不管怎么说，C++标准库是世界顶级水准的代码，用C++，当然是优先使用C++标准库的设施。这本书专门讲解C++标准库，篇幅很长，是非常好的工具书。

  下面是C++进阶类或者专题类：

  1）《Effective C++》、《Effective Modern C++》。这两本书的作者是同一个人，而且是大师。他把专家经验，深入浅出的写出来。熟读C++教科书，并不能让你可以上手干活，该踩的坑还是会踩。读专家经验是一个很好的过渡过程。

  2）《Effective STL》、《STL源码剖析》。这两本书是对STL进行深度挖掘或者剖析的，但要注意这两本书出版时间都很久了，有些条款已经不适合C++11以后的标准。

  3）《C++程序设计语言》。Bjarne Stroustrup 著，这是最权威的、最全面的C++专著。内容丰富、几乎覆盖整个C++标准。但是我个人觉得这本书更适合查缺补漏。从头到尾读实在是一个挑战。

  4）《Linux多线程服务端编程》，作者是陈硕。这本书其实主要是介绍作者开发的开源软件，这个软件对TCP服务器通信进行了封装。由于C++程序员几乎都会涉及网络通信，因此读一本这样的书是有必要的。除此以外，陈硕分享了他的一些C++工程经验，其中关于多线程编程、编译过程、日志等都有独到的见解。

   还有一本书我之前经常推荐，那就是《深入理解计算机系统》，虽然与C++没有直接关系，但是这本书详细全面介绍了计算机软硬件系统。对于想要写出高性能C++程序来说，仅仅学习C++本身是不够的，《深入理解计算机系统》是非常好的进阶教程。

   把上面的书读完需要很多时间，1年的时间肯定不够，但是是值得的。有的人追着热点走，什么流行就做什么，浮光掠影，他们到底有多少东西在工业现场真正发挥着基础设施的作用，是可以打一个问号的。把基础打好，做一些有深度的事情，虽然不光鲜，可能也没荣誉，但终究会有长远的回报。